{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1yOghQJ2CoQk_b08shirdhHVPbxObRZjA",
      "authorship_tag": "ABX9TyOEy8R4iZ18RtO4xBFh4Cln",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vloker/machine-learning/blob/main/mobilenetV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FKcaebEjqmn-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision.transforms import functional as F\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision import models\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations\n",
        "class CocoTransform:\n",
        "    def __call__(self, image, target):\n",
        "        image = F.to_tensor(image)  # Convert PIL image to tensor\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "X5AKZQPLq7DH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class\n",
        "def get_coco_dataset(img_dir, ann_file):\n",
        "    return CocoDetection(\n",
        "        root=img_dir,\n",
        "        annFile=ann_file,\n",
        "        transforms=CocoTransform()\n",
        "    )\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = get_coco_dataset(\n",
        "    img_dir=\"/content/drive/MyDrive/machine-learning/database/data-1/train\",\n",
        "    ann_file=\"/content/drive/MyDrive/machine-learning/database/data-1/train/annotations/instances_default.json\"\n",
        ")\n",
        "\n",
        "val_dataset = get_coco_dataset(\n",
        "    img_dir=\"/content/drive/MyDrive/machine-learning/database/data-1/val\",\n",
        "    ann_file=\"/content/drive/MyDrive/machine-learning/database/data-1/val/annotations/instances_default.json\"\n",
        ")\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJWbZOyIq85E",
        "outputId": "a82f10a4-98c8-4164-fd2d-d5d491edd340"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.04s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.19s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_with_mobilenet_v3(num_classes):\n",
        "    # Memuat backbone MobileNetV3 (menggunakan bobot yang telah dilatih sebelumnya)\n",
        "    backbone = models.mobilenet_v3_large(pretrained=True).features\n",
        "\n",
        "    # Tambahkan lapisan konvolusi 1x1 untuk mengubah saluran menjadi 1280\n",
        "    backbone = nn.Sequential(\n",
        "        backbone,\n",
        "        nn.Conv2d(960, 1280, kernel_size=1),  # Penyesuaian channel output\n",
        "    )\n",
        "    backbone.out_channels = 1280  # Tentukan saluran output backbone\n",
        "\n",
        "    # Mendefinisikan generator anchor untuk RPN\n",
        "    anchor_generator = AnchorGenerator(\n",
        "        sizes=((32, 64, 128, 256, 512),),  # Ukuran anchor\n",
        "        aspect_ratios=((0.5, 1.0, 2.0),) * 5  # Rasio aspek anchor\n",
        "    )\n",
        "\n",
        "    # Define ROI Pooling\n",
        "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
        "        featmap_names=['0'],  # Feature maps to consider\n",
        "        output_size=10,  # Spatial size of the output\n",
        "        sampling_ratio=2\n",
        "    )\n",
        "\n",
        "    # Create the Faster R-CNN model\n",
        "    model = FasterRCNN(\n",
        "        backbone=backbone,\n",
        "\n",
        "        num_classes=num_classes,\n",
        "        rpn_anchor_generator=anchor_generator,\n",
        "        box_roi_pool=roi_pooler\n",
        "    )\n",
        "\n",
        "    # Replace the classifier head\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "p136ay2frUcb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "num_classes = 5 # Background + sehat, bercabang, retak, bercak hitam\n",
        "model = get_model_with_mobilenet_v3(num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3MR6Nx-rZSo",
        "outputId": "3bb7276e-b4ff-440d-cdb7-6025b91a8de9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adam(params, lr=0.001, weight_decay=0.001)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
      ],
      "metadata": {
        "id": "738_ldTNriKm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def IoU(box1, box2):\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    # Compute intersection area\n",
        "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "\n",
        "    # Compute union area\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "    union_area = box1_area + box2_area - inter_area\n",
        "\n",
        "    # Compute IoU\n",
        "    return inter_area / union_area if union_area > 0 else 0"
      ],
      "metadata": {
        "id": "kp7UYysDrlOg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_with_metrics(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_pred_boxes = 0\n",
        "    total_true_boxes = 0\n",
        "    total_loss = 0.0\n",
        "    total_iou = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    loss_fn = torch.nn.SmoothL1Loss(reduction=\"sum\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            images = [img.to(device) for img in images]\n",
        "            outputs = model(images)\n",
        "\n",
        "            for output, target in zip(outputs, targets):\n",
        "                pred_boxes = output['boxes'].to(device)\n",
        "                pred_labels = output['labels'].to(device)\n",
        "                true_boxes = torch.tensor(\n",
        "                    [[x, y, x + w, y + h] for x, y, w, h in [obj['bbox'] for obj in target]],\n",
        "                    dtype=torch.float32,\n",
        "                    device=device\n",
        "                )\n",
        "                true_labels = torch.tensor([obj['category_id'] for obj in target], dtype=torch.int64, device=device)\n",
        "\n",
        "                matched_pred_indices = set()\n",
        "                matched_true_indices = set()\n",
        "\n",
        "                for pred_idx, (pb, pl) in enumerate(zip(pred_boxes, pred_labels)):\n",
        "                    for true_idx, (tb, tl) in enumerate(zip(true_boxes, true_labels)):\n",
        "                        if pred_idx in matched_pred_indices or true_idx in matched_true_indices:\n",
        "                            continue  # Avoid double counting matches\n",
        "\n",
        "                        iou_score = IoU(pb.cpu().numpy(), tb.cpu().numpy())\n",
        "                        if tl == pl and iou_score > 0.5:\n",
        "                            matched_pred_indices.add(pred_idx)\n",
        "                            matched_true_indices.add(true_idx)\n",
        "                            total_correct += 1\n",
        "                            total_iou += iou_score\n",
        "                            total_loss += loss_fn(pb, tb).item()\n",
        "                            num_batches += 1\n",
        "                            break\n",
        "\n",
        "                total_pred_boxes += len(pred_boxes)\n",
        "                total_true_boxes += len(true_boxes)\n",
        "\n",
        "    accuracy = (total_correct / total_true_boxes * 100) if total_true_boxes > 0 else 0.0\n",
        "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
        "    mean_iou = total_iou / total_correct if total_correct > 0 else 0.0\n",
        "    return avg_loss, accuracy, mean_iou"
      ],
      "metadata": {
        "id": "F1-4K9s8rnZ8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    progress = tqdm(data_loader, desc=f\"Epoch [{epoch}/{num_epochs}]\", leave=True)\n",
        "    for images, targets in progress:\n",
        "        images = [img.to(device) for img in images]\n",
        "\n",
        "        processed_targets = []\n",
        "        valid_images = []\n",
        "        for i, target in enumerate(targets):\n",
        "            boxes = []\n",
        "            labels = []\n",
        "            for obj in target:\n",
        "                bbox = obj[\"bbox\"]\n",
        "                x, y, w, h = bbox\n",
        "                if w > 0 and h > 0:\n",
        "                    boxes.append([x, y, x + w, y + h])\n",
        "                    labels.append(obj[\"category_id\"])\n",
        "\n",
        "            if boxes:\n",
        "                processed_target = {\n",
        "                    \"boxes\": torch.tensor(boxes, dtype=torch.float32).to(device),\n",
        "                    \"labels\": torch.tensor(labels, dtype=torch.int64).to(device),\n",
        "                }\n",
        "                processed_targets.append(processed_target)\n",
        "                valid_images.append(images[i])\n",
        "\n",
        "        if not processed_targets:\n",
        "            continue\n",
        "\n",
        "        images = valid_images\n",
        "        loss_dict = model(images, processed_targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += losses.item()\n",
        "        num_batches += 1\n",
        "        progress.set_postfix(loss=losses.item())\n",
        "\n",
        "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
        "    elapsed_time = time.time() - start_time\n",
        "    minutes, seconds = divmod(int(elapsed_time), 60)\n",
        "\n",
        "    return avg_loss, minutes, seconds"
      ],
      "metadata": {
        "id": "AVTRAn3VroyV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with formatted output\n",
        "num_epochs = 5\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss, train_minutes, train_seconds = train_one_epoch(model, optimizer, train_loader, device, epoch, num_epochs)\n",
        "    val_loss, val_accuracy, val_mean_iou = evaluate_with_metrics(model, val_loader, device)\n",
        "\n",
        "    print(f\"Epoch [{epoch}/{num_epochs}]\")\n",
        "    print(f\"[Train] Loss: {train_loss:.4f} | Time: {train_minutes}m {train_seconds}s\")\n",
        "    print(f\"[Validation] Loss: {val_loss:.4f} | Accuracy: {val_accuracy:.2f}% | Mean IoU: {val_mean_iou:.3f}\")\n",
        "\n",
        "    # Save the model's state dictionary after every epoch\n",
        "    model_path = f\"fasterrcnn_mobilenetV3_epoch_{epoch}.pth\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved: {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "5fx0R5Hyrrxo",
        "outputId": "baaea681-8086-46e3-9a42-556dbb164822"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1/5]:   1%|          | 1/173 [06:48<19:32:24, 408.98s/it, loss=2.58]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-338308da5684>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_minutes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_seconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mean_iou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_with_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-3c476ae7cf0f>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, num_epochs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "by8XltFa1O6B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}